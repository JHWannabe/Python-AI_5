{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfbf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6ebc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequential_model(input_shape):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(input_shape),\n",
    "            \n",
    "            layers.Conv2D(64, 3, strides=1, activation='relu', padding='same'),\n",
    "            layers.Conv2D(64, 3, strides=1, activation='relu', padding='same'),\n",
    "            layers.MaxPool2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            # 2nd\n",
    "            layers.Conv2D(128, 3, strides=1, activation='relu', padding='same'),\n",
    "            layers.Conv2D(128, 3, strides=1, activation='relu', padding='same'),\n",
    "            layers.MaxPool2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            # Classifier\n",
    "            layers.GlobalMaxPool2D(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837ea043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 128, 128, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 128, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 64, 64, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64, 64, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 128)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 277,569\n",
      "Trainable params: 277,185\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "model = get_sequential_model(input_shape)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics='accuracy'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2848643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, csv_path, fold, image_size, mode='train', shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.df = self.df[self.df['fold'] != self.fold]\n",
    "        elif self.mode == 'val':\n",
    "            self.df = self.df[self.df['fold'] == self.fold]\n",
    "        \n",
    "        #### https://github.com/tensorflow/models/issues/3134\n",
    "        #### 파일 이슈 -> 삭제\n",
    "        invalid_filename = [\n",
    "            'Egyptian_Mau_14',\n",
    "            'Egyptian_Mau_139',\n",
    "            'Egyptian_Mau_145',\n",
    "            'Egyptian_Mau_156',\n",
    "            'Egyptian_Mau_167',\n",
    "            'Egyptian_Mau_177',\n",
    "            'Egyptian_Mau_186',\n",
    "            'Egyptian_Mau_191',\n",
    "            'Abyssinian_5',\n",
    "            'Abyssinian_34',\n",
    "            'chihuahua_121',\n",
    "            'beagle_116'\n",
    "        ]\n",
    "        self.df = self.df[~self.df['file_name'].isin(invalid_filenames)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "            return math.ceil(len(self.df) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        strt = idx * self.batch_size\n",
    "        fin = (idx + 1) * self.batch_size\n",
    "        data = self.df.iloc[strt:fin]\n",
    "        batch_x, batch_y = self.get_data(data)\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "        \n",
    "    def get_data(self, data):\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for _, r in data.iterrows():\n",
    "            file_name = r['file_name']\n",
    "            image = cv2.imread(f'data/images/{file_name}.jpg')\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "            image = image / 255.\n",
    "        \n",
    "            label = int(r['species']) - 1\n",
    "            \n",
    "            batch_x.append(image)\n",
    "            batch_y.append(label)\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7efa2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'invalid_filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kfolds.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mDataGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m valid_generator \u001b[38;5;241m=\u001b[39m DataGenerator(\n\u001b[0;32m     13\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     14\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m csv_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mDataGenerator.__init__\u001b[1;34m(self, batch_size, csv_path, fold, image_size, mode, shuffle)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#### https://github.com/tensorflow/models/issues/3134\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#### 파일 이슈 -> 삭제\u001b[39;00m\n\u001b[0;32m     18\u001b[0m invalid_filename \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEgyptian_Mau_14\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEgyptian_Mau_139\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeagle_116\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m ]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;241m~\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43minvalid_filenames\u001b[49m)]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'invalid_filenames' is not defined"
     ]
    }
   ],
   "source": [
    "csv_path = 'data/kfolds.csv'\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    batch_size = 128,\n",
    "    csv_path = csv_path,\n",
    "    fold = 1,\n",
    "    image_size = 256,\n",
    "    mode = 'train',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "valid_generator = DataGenerator(\n",
    "    batch_size = 128,\n",
    "    csv_path = csv_path,\n",
    "    fold = 1,\n",
    "    image_size = 256,\n",
    "    mode = 'val',\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876ae178",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_generator\u001b[49m,\n\u001b[0;32m      3\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_generator,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      5\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m         early_stopping,\n\u001b[0;32m      7\u001b[0m         reduce_on_plateau,\n\u001b[0;32m      8\u001b[0m         model_checkpoint\n\u001b[0;32m      9\u001b[0m     ],\n\u001b[0;32m     10\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        reduce_on_plateau,\n",
    "        model_checkpoint\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838f2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# 무조건 Epoch을 많이 돌린 후 특정 시점에 멈추게 함\n",
    "# monitor : EarlyStopping의 기준이 되는 값, val_loss는 val_loss 더이상 감소되지 않을 경우 EarlyStopping을 적용\n",
    "# patience : Training이 진행됨에도 더이상 monitor되는 값의 개선이 없을 경우 몇 번의 epoch을 진행할 지 정하는 값\n",
    "# mode : monitor되는 값이 최소가 되야 하는지, 최대가 되야 하는지 설정하는 값\n",
    "# restore_best_weights : true라면 training이 끝난 후, model의 weight를 monitor하고 있던 값이 가장 좋았을 경우의 weight로 복원,\n",
    "# False라면 마지막 training이 끝난 후의 weight로 설정\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cfd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau\n",
    "# monitor : ReduceLROnPlateau의 기준이 되는 값, val_loss가 더이상 감소되지 않을 경우 ReduceLROnPlateau를 적용\n",
    "# factor : learning rate를 얼마나 감소시킬지 정하는 값. 새로운 learning rate는 기존 learning rate * factor\n",
    "# patience : training이 진행됨에도 더이상 monitor되는 값의 개선이 없을 경우 최적의 monitor 값을 기준으로 몇 번의 epoch을 진행하고\n",
    "# learning rate를 조절할 지의 값을 설정\n",
    "\n",
    "reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='min', min_lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a9a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 경로\n",
    "# 모델 경로를 '{epoch:02d}-{val_loss:.2f}.hdf5'라고 하면 앞에 명시한 문자열로 파일을 저장\n",
    "# 예: 01-0.12.h5\n",
    "# save_weights_only : True -> weights만 저장, False -> 모델, layer, weights 모두 저장\n",
    "\n",
    "filepath = '{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor = 'val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9d415",
   "metadata": {},
   "source": [
    "데이터 에러 이슈  \n",
    "https://github.com/tensorflow/models/issues/3134\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d23c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
